# DropAlert Rwanda: Predicting Student Dropouts Using Local Socio-Economic Indicators
# Capstone Project - Big Data Analytics
# Author: [Your Name]
# Date: July 31, 2025

# =============================================================================
# STEP 1: INSTALL AND IMPORT REQUIRED LIBRARIES
# =============================================================================

# Install required packages (run these in your Jupyter notebook)
# !pip install pandas numpy matplotlib seaborn scikit-learn plotly

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
import warnings
warnings.filterwarnings('ignore')

# Set plotting style
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print("‚úÖ All libraries imported successfully!")
print("üìä Starting DropAlert Rwanda Analysis...")
print("="*60)

# =============================================================================
# STEP 2: LOAD AND EXPLORE THE DATASET
# =============================================================================

print("\nüîç STEP 2: LOADING AND EXPLORING DATASET")
print("-" * 45)

# Load your actual dataset
df = pd.read_csv("DropAlert_Rwanda_Dataset.csv")

print("‚úÖ Dataset loaded successfully!")
print(f"üìä Dataset shape: {df.shape}")
print(f"üìä Dataset covers {df['Year'].min()} to {df['Year'].max()}")
print(f"üìä Number of schools: {df['School_Name'].nunique()}")
print(f"üìä Number of provinces: {df['Province'].nunique()}")
print(f"üìä Number of districts: {df['District'].nunique()}")

print("\nüßæ Available Columns in Dataset:")
for i, col in enumerate(df.columns, 1):
    print(f"  {i:2}. {col}")

print("\nüìå First 5 rows:")
print(df.head())

print("\nüîç Missing Values Check:")
missing_values = df.isnull().sum()
if missing_values.sum() == 0:
    print("‚úÖ No missing values found - dataset is clean!")
else:
    print(missing_values[missing_values > 0])

print("\nüìà Basic Dataset Statistics:")
print(df.describe())

# =============================================================================
# STEP 3: DATA PREPROCESSING AND CLEANING
# =============================================================================

print("\n" + "="*60)
print("üßπ STEP 3: DATA PREPROCESSING AND CLEANING")
print("-" * 45)

# Create a working copy of the dataset
df_clean = df.copy()

print("üîß Processing Teacher_Student_Ratio...")
# Convert Teacher_Student_Ratio from "1:40" format to numeric
df_clean['Teacher_Student_Ratio_Numeric'] = df_clean['Teacher_Student_Ratio'].str.extract('1:(\d+)').astype(int)

print("üèòÔ∏è Creating regional categories...")
# Create regional categories (Urban vs Rural based on Province)
urban_provinces = ['Kigali']  # Kigali is typically considered urban
df_clean['Region_Type'] = df_clean['Province'].apply(lambda x: 'Urban' if 'Kigali' in x else 'Rural')

print("üí∞ Converting household income to millions...")
# Convert household income to millions for easier interpretation
df_clean['Avg_Household_Income_Million_RWF'] = df_clean['Avg_Household_Income_RWF'] / 1000000

print("üìä Creating additional features...")
# Create dropout risk categories
df_clean['Dropout_Risk_Level'] = pd.cut(df_clean['Overall_Dropout_Percentage'], 
                                       bins=[0, 10, 20, 100], 
                                       labels=['Low', 'Medium', 'High'])

# Gender dropout gap
df_clean['Gender_Dropout_Gap'] = df_clean['Male_Dropout_Percentage'] - df_clean['Female_Dropout_Percentage']

# Gender completion gap
df_clean['Gender_Completion_Gap'] = df_clean['Completion_Female'] - df_clean['Completion_Male']

print("‚úÖ Data preprocessing completed!")
print(f"üìä Cleaned dataset shape: {df_clean.shape}")
print(f"üìä New columns added: 5")

# Show the distribution of dropout risk levels
print("\nüìä Dropout Risk Level Distribution:")
print(df_clean['Dropout_Risk_Level'].value_counts())

# =============================================================================
# STEP 4: EXPLORATORY DATA ANALYSIS (EDA) - PART 1: TREND ANALYSIS
# =============================================================================

print("\n" + "="*60)
print("üîç STEP 4: EXPLORATORY DATA ANALYSIS - TREND ANALYSIS")
print("-" * 45)

# Create comprehensive visualizations
fig, axes = plt.subplots(2, 3, figsize=(20, 12))
fig.suptitle('DropAlert Rwanda: Comprehensive Education Analysis Dashboard', fontsize=18, fontweight='bold')

# Plot 1: Overall dropout trends over time
yearly_dropout = df_clean.groupby('Year')['Overall_Dropout_Percentage'].mean()
axes[0,0].plot(yearly_dropout.index, yearly_dropout.values, marker='o', linewidth=3, color='red', markersize=8)
axes[0,0].fill_between(yearly_dropout.index, yearly_dropout.values, alpha=0.3, color='red')
axes[0,0].set_title('üìâ Average Dropout Rate Trend Over Time', fontsize=12, fontweight='bold')
axes[0,0].set_xlabel('Year')
axes[0,0].set_ylabel('Dropout Rate (%)')
axes[0,0].grid(True, alpha=0.3)

# Plot 2: Gender comparison in dropouts
yearly_male = df_clean.groupby('Year')['Male_Dropout_Percentage'].mean()
yearly_female = df_clean.groupby('Year')['Female_Dropout_Percentage'].mean()
axes[0,1].plot(yearly_male.index, yearly_male.values, marker='s', linewidth=3, label='Male', color='blue')
axes[0,1].plot(yearly_female.index, yearly_female.values, marker='^', linewidth=3, label='Female', color='pink')
axes[0,1].set_title('üë´ Gender-Based Dropout Trends', fontsize=12, fontweight='bold')
axes[0,1].set_xlabel('Year')
axes[0,1].set_ylabel('Dropout Rate (%)')
axes[0,1].legend()
axes[0,1].grid(True, alpha=0.3)

# Plot 3: Provincial comparison
province_dropout = df_clean.groupby('Province')['Overall_Dropout_Percentage'].mean().sort_values(ascending=False)
bars = axes[0,2].bar(range(len(province_dropout)), province_dropout.values, color='orange', alpha=0.7)
axes[0,2].set_title('üèòÔ∏è Average Dropout Rate by Province', fontsize=12, fontweight='bold')
axes[0,2].set_xlabel('Province')
axes[0,2].set_ylabel('Dropout Rate (%)')
axes[0,2].set_xticks(range(len(province_dropout)))
axes[0,2].set_xticklabels(province_dropout.index, rotation=45, ha='right')

# Add value labels on bars
for bar, value in zip(bars, province_dropout.values):
    axes[0,2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                   f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')

# Plot 4: Completion vs Dropout relationship
scatter = axes[1,0].scatter(df_clean['Completion_Total'], df_clean['Overall_Dropout_Percentage'], 
                           alpha=0.6, color='green', s=30)
axes[1,0].set_title('üìä Completion Rate vs Dropout Rate', fontsize=12, fontweight='bold')
axes[1,0].set_xlabel('Completion Rate (%)')
axes[1,0].set_ylabel('Dropout Rate (%)')
axes[1,0].grid(True, alpha=0.3)

# Add correlation coefficient
corr_coef = df_clean['Completion_Total'].corr(df_clean['Overall_Dropout_Percentage'])
axes[1,0].text(0.05, 0.95, f'Correlation: {corr_coef:.3f}', transform=axes[1,0].transAxes, 
               bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))

# Plot 5: Household Income vs Dropout
# Create income bins for better visualization
income_bins = pd.cut(df_clean['Avg_Household_Income_Million_RWF'], bins=5)
income_dropout = df_clean.groupby(income_bins)['Overall_Dropout_Percentage'].mean()
bars = axes[1,1].bar(range(len(income_dropout)), income_dropout.values, color='purple', alpha=0.7)
axes[1,1].set_title('üí∞ Dropout Rate by Household Income Level', fontsize=12, fontweight='bold')
axes[1,1].set_xlabel('Income Level (Million RWF)')
axes[1,1].set_ylabel('Dropout Rate (%)')
axes[1,1].set_xticks(range(len(income_dropout)))
income_labels = [f"{interval.left:.2f}-{interval.right:.2f}" for interval in income_dropout.index]
axes[1,1].set_xticklabels(income_labels, rotation=45, ha='right')

# Plot 6: Region Type comparison
region_dropout = df_clean.groupby('Region_Type')['Overall_Dropout_Percentage'].mean()
bars = axes[1,2].bar(region_dropout.index, region_dropout.values, 
                     color=['skyblue', 'lightcoral'], alpha=0.8)
axes[1,2].set_title('üèôÔ∏è Urban vs Rural Dropout Rates', fontsize=12, fontweight='bold')
axes[1,2].set_xlabel('Region Type')
axes[1,2].set_ylabel('Dropout Rate (%)')

# Add value labels
for bar, value in zip(bars, region_dropout.values):
    axes[1,2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                   f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.show()

# =============================================================================
# STEP 5: EXPLORATORY DATA ANALYSIS - PART 2: DETAILED INSIGHTS
# =============================================================================

print("\n" + "="*60)
print("üìä STEP 5: DETAILED INSIGHTS AND SCHOOL PERFORMANCE")
print("-" * 45)

# School performance analysis
print("üèÜ TOP 10 BEST PERFORMING SCHOOLS (Lowest Dropout Rate):")
best_schools = df_clean.groupby(['School_Name', 'Province'])['Overall_Dropout_Percentage'].mean().sort_values().head(10)
for i, ((school, province), rate) in enumerate(best_schools.items(), 1):
    print(f"   {i:2}. {school} ({province}): {rate:.2f}%")

print("\n‚ö†Ô∏è TOP 10 SCHOOLS NEEDING URGENT SUPPORT (Highest Dropout Rate):")
worst_schools = df_clean.groupby(['School_Name', 'Province'])['Overall_Dropout_Percentage'].mean().sort_values(ascending=False).head(10)
for i, ((school, province), rate) in enumerate(worst_schools.items(), 1):
    print(f"   {i:2}. {school} ({province}): {rate:.2f}%")

# Regional analysis
print(f"\nüåç REGIONAL ANALYSIS:")
regional_stats = df_clean.groupby('Region_Type').agg({
    'Overall_Dropout_Percentage': ['mean', 'std'],
    'Avg_Household_Income_Million_RWF': 'mean',
    'Teacher_Student_Ratio_Numeric': 'mean',
    'Attendance_Rate': 'mean'
}).round(2)

print(regional_stats)

# Time trend analysis
print(f"\nüìà DROPOUT RATE TRENDS:")
year_trends = df_clean.groupby('Year')['Overall_Dropout_Percentage'].agg(['mean', 'std']).round(2)
print(year_trends)

# Calculate improvement rate
first_year_rate = year_trends.iloc[0]['mean']
last_year_rate = year_trends.iloc[-1]['mean']
improvement = first_year_rate - last_year_rate
improvement_percent = (improvement / first_year_rate) * 100

print(f"\nüìä OVERALL IMPROVEMENT:")
print(f"   ‚Ä¢ Starting dropout rate ({df_clean['Year'].min()}): {first_year_rate:.2f}%")  
print(f"   ‚Ä¢ Ending dropout rate ({df_clean['Year'].max()}): {last_year_rate:.2f}%")
print(f"   ‚Ä¢ Total improvement: {improvement:.2f} percentage points")
print(f"   ‚Ä¢ Relative improvement: {improvement_percent:.1f}%")

# =============================================================================
# STEP 6: CORRELATION ANALYSIS
# =============================================================================

print("\n" + "="*60)
print("üîó STEP 6: CORRELATION ANALYSIS")
print("-" * 45)

# Select numeric columns for correlation analysis
numeric_cols = ['Overall_Dropout_Percentage', 'Male_Dropout_Percentage', 'Female_Dropout_Percentage',
                'Completion_Total', 'Completion_Male', 'Completion_Female', 'Reenrollment_Rate',
                'Attendance_Rate', 'Avg_Household_Income_Million_RWF', 'Teacher_Student_Ratio_Numeric']

correlation_matrix = df_clean[numeric_cols].corr()

# Focus on correlations with overall dropout rate
dropout_correlations = correlation_matrix['Overall_Dropout_Percentage'].sort_values(key=abs, ascending=False)

print("üéØ CORRELATIONS WITH OVERALL DROPOUT RATE:")
for variable, correlation in dropout_correlations.items():
    if variable != 'Overall_Dropout_Percentage':
        direction = "Strong" if abs(correlation) > 0.7 else "Moderate" if abs(correlation) > 0.4 else "Weak"
        sign = "Positive" if correlation > 0 else "Negative"
        print(f"   ‚Ä¢ {variable}: {correlation:.3f} ({direction} {sign})")

# Create correlation heatmap
plt.figure(figsize=(12, 10))
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
sns.heatmap(correlation_matrix, annot=True, cmap='RdYlBu_r', center=0, 
            square=True, fmt='.3f', cbar_kws={"shrink": .8}, mask=mask)
plt.title('üîó Correlation Matrix: Rwanda Education Indicators', fontsize=16, fontweight='bold')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# =============================================================================
# STEP 7: MACHINE LEARNING MODEL PREPARATION
# =============================================================================

print("\n" + "="*60)
print("ü§ñ STEP 7: MACHINE LEARNING MODEL PREPARATION")
print("-" * 45)

# Prepare data for machine learning
df_ml = df_clean.copy()

# Create binary target variable based on dropout risk
dropout_threshold = df_ml['Overall_Dropout_Percentage'].median()
df_ml['High_Dropout_Risk'] = (df_ml['Overall_Dropout_Percentage'] > dropout_threshold).astype(int)

print(f"üìä Dropout Risk Classification (Threshold: {dropout_threshold:.1f}%):")
risk_distribution = df_ml['High_Dropout_Risk'].value_counts()
print(f"   ‚Ä¢ Low Risk (0): {risk_distribution[0]} schools ({risk_distribution[0]/len(df_ml)*100:.1f}%)")
print(f"   ‚Ä¢ High Risk (1): {risk_distribution[1]} schools ({risk_distribution[1]/len(df_ml)*100:.1f}%)")

# Prepare features for the model
feature_columns = [
    'Avg_Household_Income_Million_RWF',
    'Teacher_Student_Ratio_Numeric', 
    'Attendance_Rate',
    'Reenrollment_Rate',
    'Completion_Total',
    'Male_Dropout_Percentage',
    'Female_Dropout_Percentage'
]

# Add encoded categorical features
le_province = LabelEncoder()
le_region = LabelEncoder()

df_ml['Province_Encoded'] = le_province.fit_transform(df_ml['Province'])
df_ml['Region_Type_Encoded'] = le_region.fit_transform(df_ml['Region_Type'])

feature_columns.extend(['Province_Encoded', 'Region_Type_Encoded'])

print(f"\nüéØ Selected Features for ML Model:")
for i, feature in enumerate(feature_columns, 1):
    print(f"   {i:2}. {feature}")

# Prepare X and y
X = df_ml[feature_columns]
y = df_ml['High_Dropout_Risk']

# Check for any missing values in features
print(f"\nüîç Missing values in features: {X.isnull().sum().sum()}")

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

print(f"\nüìä Data Split:")
print(f"   ‚Ä¢ Training set: {X_train.shape[0]} samples")
print(f"   ‚Ä¢ Test set: {X_test.shape[0]} samples")
print(f"   ‚Ä¢ Features: {X_train.shape[1]}")

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("‚úÖ Feature scaling completed!")

# =============================================================================
# STEP 8: MACHINE LEARNING MODEL TRAINING
# =============================================================================

print("\n" + "="*60)
print("üöÄ STEP 8: MACHINE LEARNING MODEL TRAINING")
print("-" * 45)

# Train Random Forest model
print("üå≥ Training Random Forest Classifier...")
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)
rf_model.fit(X_train_scaled, y_train)

# Make predictions
rf_predictions = rf_model.predict(X_test_scaled)
rf_probabilities = rf_model.predict_proba(X_test_scaled)[:, 1]

# Calculate metrics
rf_accuracy = accuracy_score(y_test, rf_predictions)
rf_precision = precision_score(y_test, rf_predictions)
rf_recall = recall_score(y_test, rf_predictions)
rf_f1 = f1_score(y_test, rf_predictions)

print("üå≥ RANDOM FOREST RESULTS:")
print(f"   ‚Ä¢ Accuracy:  {rf_accuracy:.3f}")
print(f"   ‚Ä¢ Precision: {rf_precision:.3f}")
print(f"   ‚Ä¢ Recall:    {rf_recall:.3f}")
print(f"   ‚Ä¢ F1-Score:  {rf_f1:.3f}")

# Train Logistic Regression model
print("\nüìà Training Logistic Regression...")
lr_model = LogisticRegression(random_state=42, max_iter=1000)
lr_model.fit(X_train_scaled, y_train)

# Make predictions
lr_predictions = lr_model.predict(X_test_scaled)
lr_accuracy = accuracy_score(y_test, lr_predictions)

print("üìà LOGISTIC REGRESSION RESULTS:")
print(f"   ‚Ä¢ Accuracy: {lr_accuracy:.3f}")

# Feature importance analysis
feature_importance = pd.DataFrame({
    'Feature': feature_columns,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

print(f"\nüéØ FEATURE IMPORTANCE RANKING:")
for i, (_, row) in enumerate(feature_importance.iterrows(), 1):
    print(f"   {i:2}. {row['Feature']}: {row['Importance']:.3f}")

# =============================================================================
# STEP 9: MODEL EVALUATION AND VISUALIZATION
# =============================================================================

print("\n" + "="*60)
print("üìä STEP 9: MODEL EVALUATION AND VISUALIZATION")
print("-" * 45)

# Create evaluation visualizations
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# Plot 1: Feature Importance
sns.barplot(data=feature_importance.head(8), x='Importance', y='Feature', palette='viridis', ax=axes[0])
axes[0].set_title('üéØ Top Feature Importance', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Importance Score')

# Plot 2: Confusion Matrix
cm = confusion_matrix(y_test, rf_predictions)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Low Risk', 'High Risk'],
            yticklabels=['Low Risk', 'High Risk'], ax=axes[1])
axes[1].set_title('üéØ Confusion Matrix', fontsize=14, fontweight='bold')
axes[1].set_ylabel('Actual')
axes[1].set_xlabel('Predicted')

# Plot 3: Model Comparison
models = ['Random Forest', 'Logistic Regression']
accuracies = [rf_accuracy, lr_accuracy]
bars = axes[2].bar(models, accuracies, color=['green', 'blue'], alpha=0.7)
axes[2].set_title('üèÜ Model Accuracy Comparison', fontsize=14, fontweight='bold')
axes[2].set_ylabel('Accuracy')
axes[2].set_ylim(0, 1)

# Add value labels on bars
for bar, accuracy in zip(bars, accuracies):
    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                f'{accuracy:.3f}', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.show()

# Detailed classification report
print("üìã DETAILED CLASSIFICATION REPORT:")
print(classification_report(y_test, rf_predictions, target_names=['Low Risk', 'High Risk']))

# =============================================================================
# STEP 10: RESULTS SUMMARY AND RECOMMENDATIONS
# =============================================================================

print("\n" + "="*60)
print("üéâ STEP 10: DROPALERT RWANDA - FINAL RESULTS & RECOMMENDATIONS")
print("="*60)

print("üìä KEY FINDINGS:")
print(f"   1. üìà Rwanda shows improvement: dropout rates decreased by {improvement:.1f} percentage points")
print(f"   2. üèòÔ∏è Regional disparity exists:")
for region, rate in region_dropout.items():
    print(f"      ‚Ä¢ {region}: {rate:.1f}% average dropout rate")
print(f"   3. ü§ñ Our ML model achieved {rf_accuracy:.1%} accuracy in predicting dropout risk")
print(f"   4. üë´ Gender gap: {abs(yearly_male.iloc[-1] - yearly_female.iloc[-1]):.1f} percentage points")

print(f"\nüéØ TOP RISK FACTORS (by ML importance):")
for i, (_, row) in enumerate(feature_importance.head(5).iterrows(), 1):
    print(f"   {i}. {row['Feature']}: {row['Importance']:.3f}")

print(f"\nüí° ACTIONABLE RECOMMENDATIONS:")
print(f"   üéØ Priority Interventions:")
print(f"      ‚Ä¢ Target schools in {province_dropout.index[0]} province (highest dropout: {province_dropout.iloc[0]:.1f}%)")
print(f"      ‚Ä¢ Focus on rural areas (dropout rate: {region_dropout['Rural']:.1f}% vs urban: {region_dropout['Urban']:.1f}%)")
print(f"      ‚Ä¢ Improve teacher-student ratios in high-risk schools")
print(f"      ‚Ä¢ Address household income disparities through scholarships")

print(f"\n   üìä Implementation Strategy:")
print(f"      ‚Ä¢ Deploy early warning system using our {rf_accuracy:.1%} accurate ML model")
print(f"      ‚Ä¢ Monitor the {worst_schools.index[0][0]} and similar high-risk schools")
print(f"      ‚Ä¢ Replicate success factors from {best_schools.index[0][0]} and other top performers")

print(f"\n   üéØ Expected Impact:")
print(f"      ‚Ä¢ Reduce overall dropout rate below {last_year_rate:.1f}%")
print(f"      ‚Ä¢ Close rural-urban gap by 50%")
print(f"      ‚Ä¢ Prevent an estimated {int(len(df_ml[df_ml['High_Dropout_Risk']==1]) * 0.3)} dropouts annually")

# Create predictions for all schools and save results
df_ml['Dropout_Risk_Prediction'] = rf_model.predict(scaler.transform(X))
df_ml['Dropout_Risk_Probability'] = rf_model.predict_proba(scaler.transform(X))[:, 1]

# Save enhanced dataset for Power BI
output_columns = ['Province', 'District', 'School_Name', 'Year', 'Overall_Dropout_Percentage',
                 'Male_Dropout_Percentage', 'Female_Dropout_Percentage', 'Completion_Total',
                 'Attendance_Rate', 'Avg_Household_Income_RWF', 'Region_Type',
                 'Dropout_Risk_Level', 'Dropout_Risk_Prediction', 'Dropout_Risk_Probability']

output_data = df_ml[output_columns].copy()
output_data.to_csv('DropAlert_Rwanda_Analysis_Results.csv', index=False)

print(f"\nüíæ OUTPUTS GENERATED:")
print(f"   ‚úÖ Enhanced dataset saved: 'DropAlert_Rwanda_Analysis_Results.csv'")
print(f"   ‚úÖ Contains {len(output_data)} records with ML predictions")
print(f"   ‚úÖ Ready for Power BI dashboard import")

print(f"\nüéä ANALYSIS COMPLETE!")
print(f"üìã Status: Ready for Power BI Dashboard Development")
print(f"üèÜ Model Performance: {rf_accuracy:.1%} accuracy, {rf_f1:.3f} F1-score")
print("="*60)